{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer,  AutoConfig, AutoModel, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup, DistilBertModel\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from itertools import islice, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_token_len\": 50,\n",
    "    \"batch_size\": 512,\n",
    "    \"n_epochs\": 15,\n",
    "    \"gpus\": 1,\n",
    "    \"early_stop_patience\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"tokenizer_model_name\": \"roberta-base\",\n",
    "    \"val_size\": 0.2,\n",
    "    \"lr\": 1e-5,\n",
    "    \"base_dir\": r\".\\base_dir\",\n",
    "    \"base_save_name\": \"roberta_base_full\",\n",
    "    \"best_save_name\": \"roberta_best_full.pt\",\n",
    "    \"non_vendor_columns\": [\"ID\", \"vendor_id\", 'Vendor_Name', 'Description', 'Cat Code Mod',\n",
    "       'Category', 'desc_expanded', 'description_cleaned_raw',\n",
    "       'description_cleaned', 'encoded_category'], #source\n",
    "    \"include_others\": True,\n",
    "    \"others_percentage\": 1.0,\n",
    "    \"n_samp\": 10,\n",
    "    \"eps\": 1e-6,\n",
    "    \"unc_rate\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Random Seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# setting up the device type\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"D:\\workspace\\Rishabh\\Uncertainty_NN\\Rebate_Data\\2_non_vendor_7000_20.csv\")  \n",
    "train_data['GT'], _ = pd.factorize(train_data['encoded_category'])\n",
    "num_classes = train_data['GT'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(params['tokenizer_model_name'])\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the custom dataset class here\n",
    "class TorchDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, data, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        raw_description = data_row.cleaned_desc \n",
    "        labels = data_row.GT    #encoded_category \n",
    "        uid = data_row.uid\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            raw_description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return_item = dict(\n",
    "            uid= torch.Tensor([uid]).long().squeeze(),\n",
    "            input_ids= encoding['input_ids'].flatten(),\n",
    "            attention_mask= encoding[\"attention_mask\"].flatten(),\n",
    "            labels= torch.Tensor([labels]).long().squeeze()\n",
    "        )\n",
    "        return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Layer 1 Output: torch.Size([1, 20])\n",
      "Extended Layer 2 Output: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 20)  # Assuming 10 output classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ExtendableNet(nn.Module):\n",
    "\tdef __init__(self, base_model):\n",
    "\t\tsuper(ExtendableNet, self).__init__()\n",
    "\t\tself.base_model = base_model\n",
    "\n",
    "\t\t# Get the modules before the last layer (assuming output layer is last)\n",
    "\t\tself.pre_output_modules = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\t\t# Define the new linear layer\n",
    "\t\tself.extended_layer = nn.Linear(self.get_pre_output_features(), \n",
    "\t\t\t\t\t\t\t\t\t\tlist(self.base_model.children())[-1].out_features)  # Same as logits dimension\n",
    "\n",
    "\tdef get_pre_output_features(self):\n",
    "\t\t\"\"\"\n",
    "\t\tExtracts the input feature size of the logits layer.\n",
    "\n",
    "\t\tThis function iterates through the modules of the base model\n",
    "\t\tand returns the number of features before the last layer.\n",
    "\t\t\"\"\"\n",
    "\t\tmodules = [mod for name, mod in self.base_model.named_children()]\n",
    "\t\tlast_non_linear_layer = modules[-1]\n",
    "\t\tif isinstance(last_non_linear_layer, nn.Linear):\n",
    "\t\t\treturn last_non_linear_layer.in_features  # Linear layer\n",
    "\t\telse:\n",
    "\t\t\t# Handle cases where the last layer is not a linear layer (e.g., activation)\n",
    "\t\t\treturn last_non_linear_layer.out_features  # Assuming final output dimension\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Pass input through all layers except the last\n",
    "\t\tpre_output = self.pre_output_modules(x)\n",
    "\n",
    "\t\t# Reshape pre_output to a vector (1xN)\n",
    "\t\tpre_output = pre_output.view(pre_output.shape[0], -1)\n",
    "\n",
    "\t\t# Pass the reshaped output through the extended layer\n",
    "\t\textended_output = self.extended_layer(pre_output)\n",
    "\n",
    "\t\t# Get the final output from the base model (logits)\n",
    "\t\tbase_output = self.base_model(x)\n",
    "\n",
    "\t\t# Return both outputs (original and extended)\n",
    "\t\treturn extended_output, base_output\n",
    "\n",
    "# Example usage: assuming your base model is defined as `BaseNet`\n",
    "base_model = OriginalModel()  # Replace with the actual model class\n",
    "extended_model = ExtendableNet(base_model)\n",
    "\n",
    "data = torch.randn(1,784)\n",
    "# Pass your data through the extended model\n",
    "extended_output1, extended_output2 = extended_model(data)\n",
    "\n",
    "print(\"Extended Layer 1 Output:\", extended_output1.shape)  # Shape of the output of the first extended layer\n",
    "print(\"Extended Layer 2 Output:\", extended_output2.shape)  # Shape of the output of the second extended layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExtendedCELoss(nn.Module):\n",
    "    \"\"\" use modified CE loss for variance calculation with UncertainLinear network \"\"\"\n",
    "    def forward(self, out:torch.Tensor, y:torch.Tensor, n_samp:int=10) -> torch.Tensor:\n",
    "        f = nn.CrossEntropyLoss()\n",
    "        logit, sigma = out   \n",
    "        dist = torch.distributions.Normal(logit, torch.exp(sigma))\n",
    "        mc_logs = dist.rsample((n_samp,))\n",
    "        loss = 0.\n",
    "\n",
    "        for mc_log in mc_logs:\n",
    "            loss += f(mc_log, y)\n",
    "            \n",
    "        loss /= n_samp\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, n_samp:int=25, is_target = True):                              \n",
    "    \"\"\" This function predicts the model and data uncertainty where samples are drawn from the target distribution for uncertainty prediction\n",
    "    \n",
    "    Input:\n",
    "        model: model object instance \n",
    "        x (tuple): (input_ids, attention_mask)\n",
    "        n_samp (int) :  number of samples used for uncertainty prediction.\n",
    "        is_target (Boolean) : If it is True, samples are drawn from target distribution for uncertainty estimation else the same input is passed multiple times to get different predictions for uncertainty estimation\n",
    "\n",
    "    Return : \n",
    "        epistemic (float): model uncertainty \n",
    "        aleatpry (float) : data uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    if is_target: \n",
    "        logit, sigma = model.forward(x[0], x[1])\n",
    "        dist = torch.distributions.Normal(logit, torch.exp(sigma))\n",
    "        mc_logs = dist.rsample((n_samp,))\n",
    "        probits = torch.sigmoid(mc_logs)\n",
    "        epistemic = probits.var(dim=0, unbiased=True)\n",
    "        aleatory = torch.exp(sigma)\n",
    "        return epistemic, aleatory\n",
    "    \n",
    "    else:\n",
    "        out = [model.forward(x[0], x[1]) for _ in range(n_samp)]\n",
    "        logits = torch.stack([o[0] for o in out]).detach().cpu()   #shape = (n_samp, 512, 10)\n",
    "        sigmas = torch.stack([o[1] for o in out]).detach().cpu()\n",
    "        probits = torch.sigmoid(logits)\n",
    "        epistemic = probits.var(dim=0, unbiased=True)\n",
    "        aleatory = torch.exp(sigmas).mean(dim=0)\n",
    "        return epistemic, aleatory\n",
    "\n",
    "\n",
    "def get_metrics(model, x, y, n_samp:int, eps:float): \n",
    "    ''' This function helps us getting the epistemic, aleatory and scibilic uncertainty(epistemic/aleatoric) values '''\n",
    "\n",
    "    state = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ep, al = predict(model, x, n_samp)\n",
    "        sb = ep / (al + eps)\n",
    "        eu, au, su = ep.cpu().numpy().mean(), al.cpu().numpy().mean(), sb.cpu().numpy().mean()\n",
    "    model.train(state)\n",
    "\n",
    "    return eu, au, su     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = train_data[train_data.set == 'TRAIN']\n",
    "val_batch = train_data[train_data.set != 'TRAIN']\n",
    "\n",
    "train_dataloader = DataLoader(TorchDataset(train_batch, tokenizer, max_token_len=params[\"max_token_len\"]), batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(TorchDataset(val_batch, tokenizer, max_token_len=params[\"max_token_len\"]), batch_size=params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = ExtendedCELoss()\n",
    "model = ExtendableNet(num_classes, params[\"dropout\"])\n",
    "optimizer = Adam(model.parameters(), lr=0.00005)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "t_ep_unc, t_en_unc, t_al_unc, t_sb_unc = [], [], [], []\n",
    "v_ep_unc, v_en_unc, v_al_unc, v_sb_unc = [], [], [], []\n",
    "n_batches = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_val_acc = -1\n",
    "patience = params[\"early_stop_patience\"]\n",
    "triggertimes = 0\n",
    "best_val_acc = -1\n",
    "\n",
    "for t in range(1, params['n_epochs']+1):\n",
    "    # training\n",
    "    t_losses, t_ep, t_al, t_sb = [], [], [], [] #t_en,\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    avg_batch_acc = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i,item_dict in enumerate(tqdm(train_dataloader)):\n",
    "        train_label = item_dict['labels'].to(device)\n",
    "        train_input_ids = item_dict['input_ids'].to(device)\n",
    "        train_attention_mask = item_dict['attention_mask'].to(device)\n",
    "        train_uid = item_dict['uid'].to(device)\n",
    "        output = model(train_input_ids, train_attention_mask) #input: 256,1 train_label:256\n",
    "        out, var= output         \n",
    "\n",
    "        batch_loss = criterion(output, train_label)\n",
    "        \n",
    "        total_loss_train += batch_loss\n",
    "        \n",
    "        acc = (out.argmax(dim=1) == train_label).sum().item()  \n",
    "        \n",
    "        total_acc_train += acc\n",
    "\n",
    "        avg_batch_acc.append(acc/len(train_batch))\n",
    "\n",
    "        \n",
    "        t_losses.append(batch_loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()     #ep : 512x10\n",
    "        if i % params['unc_rate'] == 0:\n",
    "            ep, al, sb = get_metrics(model, (train_input_ids, train_attention_mask), train_label, params['n_samp'], params['eps'])   \n",
    "            t_ep.append(ep)  \n",
    "            t_al.append(al); t_sb.append(sb)\n",
    "    train_losses.append(t_losses)\n",
    "    t_ep_unc.append(t_ep); \n",
    "    t_al_unc.append(t_al); t_sb_unc.append(t_sb)\n",
    "\n",
    "    print('********** VALIDATION STARTS **********')\n",
    "    # validation\n",
    "    v_losses, v_ep, v_en, v_al, v_sb = [], [], [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_item_dict in tqdm(val_dataloader):\n",
    "            val_label = val_item_dict['labels'].to(device)\n",
    "            val_input_ids = val_item_dict['input_ids'].to(device)\n",
    "            val_attention_mask = val_item_dict['attention_mask'].to(device)\n",
    "            val_uid = val_item_dict['uid'].to(device) \n",
    "            \n",
    "            output = model(val_input_ids, val_attention_mask)\n",
    "            out, var = output    \n",
    "    \n",
    "            batch_loss = criterion(output, val_label)\n",
    "            total_loss_val += batch_loss.item()\n",
    "            \n",
    "            acc = (out.argmax(dim=1) == val_label).sum().item()   \n",
    "            total_acc_val += acc\n",
    "\n",
    "            ep, al, sb = get_metrics(model, (val_input_ids, val_attention_mask), val_label, params['n_samp'], params['eps'])\n",
    "            v_losses.append(batch_loss.item())\n",
    "            v_ep.append(ep) \n",
    "            v_al.append(al); v_sb.append(sb)\n",
    "        valid_losses.append(v_losses)\n",
    "        v_ep_unc.append(v_ep)\n",
    "        v_al_unc.append(v_al); v_sb_unc.append(v_sb)\n",
    "\n",
    "\n",
    "    print(f'Epochs: {t} | Train Loss: {total_loss_train / len(train_batch): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_batch): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_batch): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_batch): .3f}\\\n",
    "                | Avg Batch Acc: {np.mean(avg_batch_acc): .3f},  TL: {np.mean(t_losses):.3f}, VL: {np.mean(v_losses): .3f}, tEU: {np.mean(t_ep): .3f}, vEU: {np.mean(v_ep): .3f} | tAU: {np.mean(t_al): .3f}, vAU: {np.mean(v_al): .3f}')\n",
    "\n",
    "    if not np.all(np.isfinite(t_losses)): \n",
    "        raise RuntimeError('NaN or Inf in training loss, cannot recover. Exiting.')\n",
    "   \n",
    "\n",
    "    if total_acc_val > best_val_acc:\n",
    "        best_val_acc = total_acc_val\n",
    "\n",
    "        save_name = os.path.join(params[\"base_dir\"], params[\"best_save_name\"])\n",
    "        \n",
    "        torch.save({'epoch': t,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),'Train loss': total_loss_train/len(train_batch),\n",
    "                    'Train Accuracy': total_acc_train / len(train_batch),'Val loss': total_loss_val/len(val_batch),'Val Accuracy': total_acc_val / len(val_batch),\n",
    "                    't_ep_unc': t_ep_unc,  't_al_unc': t_al_unc, 't_sb_unc': t_sb_unc, 'v_ep_unc': v_ep_unc, 'v_al_unc': v_al_unc, 'v_sb_unc': v_sb_unc}, save_name)   \n",
    "\n",
    "\n",
    "\n",
    "    save_name = os.path.join(params[\"base_dir\"], str(t) + \"_\" + f'{total_acc_val / len(val_batch): .3f}' + \".pt\")\n",
    "    \n",
    "    torch.save({'epoch': t,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),'Train loss': total_loss_train/len(train_batch),\n",
    "                    'Train Accuracy': total_acc_train / len(train_batch),'Val loss': total_loss_val/len(val_batch),'Val Accuracy': total_acc_val / len(val_batch),\n",
    "                    't_ep_unc': t_ep_unc, 't_al_unc': t_al_unc, 't_sb_unc': t_sb_unc, \n",
    "                    'v_ep_unc': v_ep_unc, 'v_al_unc': v_al_unc, 'v_sb_unc': v_sb_unc}, save_name)  \n",
    "    \n",
    "    if last_val_acc >= total_acc_val:\n",
    "        print(last_val_acc, total_acc_val)\n",
    "        triggertimes += 1\n",
    "        print('Trigger Times:', triggertimes)\n",
    "        \n",
    "        if triggertimes >= patience:\n",
    "            print('Early stopping!\\n')\n",
    "            break\n",
    "    else:\n",
    "        triggertimes = 0\n",
    "        \n",
    "    last_val_acc = total_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('t_ep_unc.csv', t_ep_unc)\n",
    "np.savetxt('t_al_unc.csv', t_al_unc)\n",
    "np.savetxt('t_sb_unc.csv', t_sb_unc)\n",
    "np.savetxt('train_losses.csv', train_losses)\n",
    "np.savetxt('v_ep_unc.csv', v_ep_unc)\n",
    "np.savetxt('v_al_unc.csv', v_al_unc)\n",
    "np.savetxt('v_sb_unc.csv', v_sb_unc)\n",
    "np.savetxt('valid_losses.csv', valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_losses(train, valid):\n",
    "    '''\n",
    "    This function helps us get the dataframe with loss values during each phase(train or validation) for each epoch\n",
    "    '''\n",
    "    out = {'epoch': [], 'type': [], 'value': [], 'phase': []}\n",
    "    for i, (tl,vl) in enumerate(zip(train,valid),1):\n",
    "        for tli in tl:\n",
    "            out['epoch'].append(i)\n",
    "            out['type'].append('loss')\n",
    "            out['value'].append(tli)\n",
    "            out['phase'].append('train')\n",
    "        for vli in vl:\n",
    "            out['epoch'].append(i)\n",
    "            out['type'].append('loss')\n",
    "            out['value'].append(vli)\n",
    "            out['phase'].append('valid')\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def tidy_uncertainty(ep, al, sb):\n",
    "    '''\n",
    "    This function helps us get the dataframe with epistemic and aleatory uncertainty values during each phase(train or validation) for each epoch\n",
    "    '''\n",
    "    out = {'epoch': [], 'type': [], 'value': [], 'phase': []}\n",
    "    for i, (epi, ali, sbi) in enumerate(zip(ep, al, sb)):\n",
    "        phase = 'train' if i == 0 else 'valid'\n",
    "        for j, (epij,alij,sbij) in enumerate(zip(epi,ali,sbi),1):\n",
    "            for epijk in epij:\n",
    "                out['epoch'].append(j)\n",
    "                out['type'].append('epistemic')\n",
    "                out['value'].append(epijk)\n",
    "                out['phase'].append(phase)\n",
    "            for alijk in alij:\n",
    "                out['epoch'].append(j)\n",
    "                out['type'].append('aleatory')\n",
    "                out['value'].append(alijk)\n",
    "                out['phase'].append(phase)\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tidy_losses(train_losses, valid_losses)\n",
    "uncert = tidy_uncertainty((t_ep_unc, v_ep_unc), \n",
    "                          (t_al_unc, v_al_unc), \n",
    "                          (t_sb_unc, v_sb_unc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1,1,figsize=(12, 8),sharey=True)\n",
    "sns.lineplot(x='epoch',y='value',hue='phase',data=losses,ax=ax1,lw=3);  #,ci='sd'\n",
    "ax1.set_title('Losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of epistemic uncertainty value during training.\n",
    "epistem_uncert = uncert[uncert['type']== 'epistemic']\n",
    "f, ax1 = plt.subplots(1,1,figsize=(12,8),sharey=True) \n",
    "sns.lineplot(x='epoch', y='value', style='phase', ci='sd',data= epistem_uncert, ax=ax1, lw=3)\n",
    "ax1.set_title('Epistemic Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1,1,figsize=(12,8),sharey=True) \n",
    "# if use_log: ax1.set(yscale='log')\n",
    "sns.lineplot(x='epoch',y='value',hue='type',style='phase',ci='sd',data=uncert,ax=ax1,lw=3);\n",
    "ax1.set_title('Uncertainty')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
