{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\win\\rishabh_uncertainty\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\conda_envs\\win\\rishabh_uncertainty\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer,  AutoConfig, AutoModel, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup, DistilBertModel\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (NVIDIA GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Get information about each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_info = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_info.name}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_token_len\": 50,\n",
    "    \"batch_size\": 512,\n",
    "    \"n_epochs\": 15,\n",
    "    \"gpus\": 1,\n",
    "    \"early_stop_patience\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"tokenizer_model_name\": \"roberta-base\",\n",
    "    \"val_size\": 0.2,\n",
    "    \"logger_filename\": 'rebate-robert-logger',\n",
    "    \"category_encoder_filename\": \"Category_Encoder_Roberta_Others.pkl\",\n",
    "    \"lr\": 1e-5,\n",
    "    \"n_samp\": 10,\n",
    "    \"eps\": 1e-6,\n",
    "    \"unc_rate\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Random Seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# setting up the device type\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned_desc</th>\n",
       "      <th>vendor_category</th>\n",
       "      <th>encoded_category</th>\n",
       "      <th>source_2</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>749918</td>\n",
       "      <td>Set_3</td>\n",
       "      <td>SYMMONS SC-3 RENEWABLE SEAT</td>\n",
       "      <td>ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS</td>\n",
       "      <td>43</td>\n",
       "      <td>new</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>619331</td>\n",
       "      <td>Set_3</td>\n",
       "      <td>600V 60A HUBBELLOCK CONNECTOR</td>\n",
       "      <td>ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS</td>\n",
       "      <td>43</td>\n",
       "      <td>new</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>595852</td>\n",
       "      <td>Set_3</td>\n",
       "      <td>NS) S91-605B FOOT PEDAL AND HOSE FOR SPEED ROO...</td>\n",
       "      <td>ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS</td>\n",
       "      <td>43</td>\n",
       "      <td>new</td>\n",
       "      <td>VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>593328</td>\n",
       "      <td>Set_3</td>\n",
       "      <td>NUMA 060271 BIT QL60 6.000 CC H</td>\n",
       "      <td>ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS</td>\n",
       "      <td>43</td>\n",
       "      <td>new</td>\n",
       "      <td>VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>738849</td>\n",
       "      <td>Set_1</td>\n",
       "      <td>ROPE SYNTH 3/8X50 ROLL</td>\n",
       "      <td>ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS</td>\n",
       "      <td>43</td>\n",
       "      <td>og</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid source                                       cleaned_desc  \\\n",
       "1   749918  Set_3                        SYMMONS SC-3 RENEWABLE SEAT   \n",
       "7   619331  Set_3                      600V 60A HUBBELLOCK CONNECTOR   \n",
       "8   595852  Set_3  NS) S91-605B FOOT PEDAL AND HOSE FOR SPEED ROO...   \n",
       "9   593328  Set_3                    NUMA 060271 BIT QL60 6.000 CC H   \n",
       "10  738849  Set_1                             ROPE SYNTH 3/8X50 ROLL   \n",
       "\n",
       "                                    vendor_category  encoded_category  \\\n",
       "1   ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS                43   \n",
       "7   ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS                43   \n",
       "8   ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS                43   \n",
       "9   ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS                43   \n",
       "10  ELECTRICAL BREAKERS, BOXES, FUSES, AND FITTINGS                43   \n",
       "\n",
       "   source_2   set  \n",
       "1       new  TEST  \n",
       "7       new  TEST  \n",
       "8       new   VAL  \n",
       "9       new   VAL  \n",
       "10       og  TEST  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking up the datapoints that were not used while updation of the model parameters during training.\n",
    "train_data = pd.read_csv(r\"D:\\workspace\\Rishabh\\Uncertainty_NN\\Rebate_Data\\1_non_vendor_15000_10.csv\")  #\"D:\\workspace\\karamjit\\rebate\\model_data\\final_train_data_uid.csv\"\n",
    "val_batch = train_data[train_data.set != 'TRAIN']\n",
    "val_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3586599610.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_batch['encoded_category'], _ = pd.factorize(val_batch['encoded_category'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned_desc</th>\n",
       "      <th>vendor_category</th>\n",
       "      <th>encoded_category</th>\n",
       "      <th>source_2</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>466482</td>\n",
       "      <td>Set_1</td>\n",
       "      <td>412-075-000 SLANT FIN PILOT</td>\n",
       "      <td>WARM AIR AND IGNITION CONTROLS</td>\n",
       "      <td>9</td>\n",
       "      <td>og</td>\n",
       "      <td>VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>703203</td>\n",
       "      <td>Set_1</td>\n",
       "      <td>Q340A1082 THERMOCOUPLE 30\"</td>\n",
       "      <td>WARM AIR AND IGNITION CONTROLS</td>\n",
       "      <td>9</td>\n",
       "      <td>og</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>608096</td>\n",
       "      <td>Set_3</td>\n",
       "      <td>0130F00008 GOODMAN IGNITER -</td>\n",
       "      <td>WARM AIR AND IGNITION CONTROLS</td>\n",
       "      <td>9</td>\n",
       "      <td>new</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>730702</td>\n",
       "      <td>Set_1</td>\n",
       "      <td>3761801 PILOT ASSEMBLY NAT GAS</td>\n",
       "      <td>WARM AIR AND IGNITION CONTROLS</td>\n",
       "      <td>9</td>\n",
       "      <td>og</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>681423</td>\n",
       "      <td>Set_1</td>\n",
       "      <td>LIMIT DISC 250F-210F 40 DEG</td>\n",
       "      <td>WARM AIR AND IGNITION CONTROLS</td>\n",
       "      <td>9</td>\n",
       "      <td>og</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid source                    cleaned_desc  \\\n",
       "149995  466482  Set_1     412-075-000 SLANT FIN PILOT   \n",
       "149996  703203  Set_1      Q340A1082 THERMOCOUPLE 30\"   \n",
       "149997  608096  Set_3    0130F00008 GOODMAN IGNITER -   \n",
       "149998  730702  Set_1  3761801 PILOT ASSEMBLY NAT GAS   \n",
       "149999  681423  Set_1     LIMIT DISC 250F-210F 40 DEG   \n",
       "\n",
       "                       vendor_category  encoded_category source_2   set  \n",
       "149995  WARM AIR AND IGNITION CONTROLS                 9       og   VAL  \n",
       "149996  WARM AIR AND IGNITION CONTROLS                 9       og  TEST  \n",
       "149997  WARM AIR AND IGNITION CONTROLS                 9      new  TEST  \n",
       "149998  WARM AIR AND IGNITION CONTROLS                 9       og  TEST  \n",
       "149999  WARM AIR AND IGNITION CONTROLS                 9       og  TEST  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batch['encoded_category'], _ = pd.factorize(val_batch['encoded_category'])\n",
    "val_batch.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = val_batch['encoded_category'].nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(params['tokenizer_model_name'])\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        raw_description = data_row.cleaned_desc \n",
    "        labels = data_row.encoded_category \n",
    "        uid = data_row.uid\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            raw_description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return_item = dict(\n",
    "            uid = torch.Tensor([uid]).long().squeeze(),\n",
    "            input_ids=encoding['input_ids'].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=torch.Tensor([labels]).long().squeeze()\n",
    "        )\n",
    "        return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 20)  # Assuming 10 output classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ExtendableNet(nn.Module):\n",
    "\tdef __init__(self, base_model):\n",
    "\t\tsuper(ExtendableNet, self).__init__()\n",
    "\t\tself.base_model = base_model\n",
    "\n",
    "\t\t# Get the modules before the last layer (assuming output layer is last)\n",
    "\t\tself.pre_output_modules = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "\t\t# Define the new linear layer\n",
    "\t\tself.extended_layer = nn.Linear(self.get_pre_output_features(), \n",
    "\t\t\t\t\t\t\t\t\t\tlist(self.base_model.children())[-1].out_features)  # Same as logits dimension\n",
    "\n",
    "\tdef get_pre_output_features(self):\n",
    "\t\t\"\"\"\n",
    "\t\tExtracts the input feature size of the logits layer.\n",
    "\n",
    "\t\tThis function iterates through the modules of the base model\n",
    "\t\tand returns the number of features before the last layer.\n",
    "\t\t\"\"\"\n",
    "\t\tmodules = [mod for name, mod in self.base_model.named_children()]\n",
    "\t\tlast_non_linear_layer = modules[-1]\n",
    "\t\tif isinstance(last_non_linear_layer, nn.Linear):\n",
    "\t\t\treturn last_non_linear_layer.in_features  # Linear layer\n",
    "\t\telse:\n",
    "\t\t\t# Handle cases where the last layer is not a linear layer (e.g., activation)\n",
    "\t\t\treturn last_non_linear_layer.out_features  # Assuming final output dimension\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Pass input through all layers except the last\n",
    "\t\tpre_output = self.pre_output_modules(x)\n",
    "\n",
    "\t\t# Reshape pre_output to a vector (1xN)\n",
    "\t\tpre_output = pre_output.view(pre_output.shape[0], -1)\n",
    "\n",
    "\t\t# Pass the reshaped output through the extended layer\n",
    "\t\textended_output = self.extended_layer(pre_output)\n",
    "\n",
    "\t\t# Get the final output from the base model (logits)\n",
    "\t\tbase_output = self.base_model(x)\n",
    "\n",
    "\t\t# Return both outputs (original and extended)\n",
    "\t\treturn base_output, extended_output\n",
    "\n",
    "# Example usage: assuming your base model is defined as `BaseNet`\n",
    "base_model = OriginalModel()  # Replace with your actual model class\n",
    "extended_model = ExtendableNet(base_model)\n",
    "\n",
    "data = torch.randn(1,784)\n",
    "# Pass your data through the extended model\n",
    "extended_output1, extended_output2 = extended_model(data)\n",
    "\n",
    "print(\"Extended Layer 1 Output:\", extended_output1.shape)  # Shape of the output of the first extended layer\n",
    "print(\"Extended Layer 2 Output:\", extended_output2.shape)  # Shape of the output of the second extended layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert_flavor): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear_1): Linear(in_features=768, out_features=500, bias=True)\n",
       "  (linear_2): Linear(in_features=500, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc_mean): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_variance): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (clf_mean): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_checkpoint_path = (r\".\\roberta_best_full.pt\")\n",
    "\n",
    "model = ExtendableNet(num_classes, params[\"dropout\"])\n",
    "model.load_state_dict(torch.load(model_checkpoint_path)[\"model_state_dict\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedCELoss(nn.Module):\n",
    "    \"\"\" use modified BCE loss for variance calculation with UncertainLinear network \"\"\"\n",
    "    def forward(self, out:torch.Tensor, y:torch.Tensor, n_samp:int=10) -> torch.Tensor:\n",
    "        f = nn.CrossEntropyLoss()\n",
    "        logit, sigma = out   \n",
    "        dist = torch.distributions.Normal(logit, torch.exp(sigma))\n",
    "        mc_logs = dist.rsample((n_samp,))\n",
    "        loss = 0.\n",
    "\n",
    "        for mc_log in mc_logs:\n",
    "            loss += f(mc_log, y)\n",
    "            \n",
    "        loss /= n_samp\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = ExtendedCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, n_samp:int=25, is_target = True):                              \n",
    "    \"\"\" This function predicts the model and data uncertainty where samples are drawn from the target distribution for uncertainty prediction\n",
    "    \n",
    "    Input:\n",
    "        model: model object instance \n",
    "        x (tuple): (input_ids, attention_mask)\n",
    "        n_samp (int) :  number of samples used for uncertainty prediction.\n",
    "        is_target (Boolean) : If it is True, samples are drawn from target distribution for uncertainty estimation else the same input is passed multiple times to get different predictions for uncertainty estimation\n",
    "\n",
    "    Return : \n",
    "        epistemic (float): model uncertainty \n",
    "        aleatpry (float) : data uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    if is_target: \n",
    "        logit, sigma = model.forward(x[0], x[1])\n",
    "        dist = torch.distributions.Normal(logit, torch.exp(sigma))\n",
    "        mc_logs = dist.rsample((n_samp,))\n",
    "        probits = torch.sigmoid(mc_logs)\n",
    "        epistemic = probits.var(dim=0, unbiased=True)\n",
    "        aleatory = torch.exp(sigma)\n",
    "        return epistemic, aleatory\n",
    "    \n",
    "    else:\n",
    "        out = [model.forward(x[0], x[1]) for _ in range(n_samp)]\n",
    "        logits = torch.stack([o[0] for o in out]).detach().cpu()   #shape = (n_samp, 512, 10)\n",
    "        sigmas = torch.stack([o[1] for o in out]).detach().cpu()\n",
    "        probits = torch.sigmoid(logits)\n",
    "        epistemic = probits.var(dim=0, unbiased=True)\n",
    "        aleatory = torch.exp(sigmas).mean(dim=0)\n",
    "        return epistemic, aleatory\n",
    "\n",
    "\n",
    "def get_metrics(model, x, y, n_samp:int, eps:float): \n",
    "    ''' This function helps us getting the epistemic, aleatory and scibilic uncertainty(epistemic/aleatoric) values '''\n",
    "\n",
    "    state = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ep, al = predict(model, x, n_samp, is_target = True)\n",
    "        sb = ep / (al + eps)\n",
    "        eu, au, su = ep.cpu().numpy().mean(), al.cpu().numpy().mean(), sb.cpu().numpy().mean()\n",
    "    model.train(state)\n",
    "\n",
    "    return eu, au, su     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, df):\n",
    "    '''\n",
    "    This function takes the model instance and the input dataframe as an input and does the inference. \n",
    "    It calculates the corresponding predictions, prediction_probability and uncertainty values for each datapoint.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    all_output = []\n",
    "    # all_labels = []\n",
    "\n",
    "    dataloader = DataLoader(TorchDataset(df, tokenizer, max_token_len=params[\"max_token_len\"]), batch_size=params[\"batch_size\"])\n",
    "    t_ep, t_al, t_sb = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_item_dict in tqdm(dataloader):\n",
    "            val_label = val_item_dict['labels'].to(device)\n",
    "            val_input_ids = val_item_dict['input_ids'].to(device)\n",
    "            val_attention_mask = val_item_dict['attention_mask'].to(device)\n",
    "            # val_vendor_vector = val_item_dict['vendor_vector'].to(device)\n",
    "            out, var = model(val_input_ids, val_attention_mask) \n",
    "            output = nn.Softmax(dim=1)(out)\n",
    "            all_output.append(output)\n",
    "\n",
    "            ep, al, sb = get_metrics(model, (val_input_ids, val_attention_mask), val_label, params['n_samp'], params['eps'])\n",
    "            \n",
    "            t_ep.extend(ep)            \n",
    "            t_al.extend(al)\n",
    "            t_sb.extend(sb)\n",
    "\n",
    "    ao = []\n",
    "    for i in all_output:\n",
    "        ao.append(i.cpu().numpy())\n",
    "    ao = np.vstack(ao)\n",
    "    ao.argmax(axis = 1).min(), ao.argmax(axis = 1).max()\n",
    "\n",
    "    df['Prediction'] = ao.argmax(axis = 1)\n",
    "    df['Prediction_Prob'] = ao.max(axis=1)\n",
    "    df['Ep_Unc'] = t_ep\n",
    "    df['Alea_Unc'] = t_al\n",
    "    df['Sb_Unc'] = t_sb\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:46<00:00,  1.11it/s]\n",
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3239121107.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Prediction'] = ao.argmax(axis = 1)\n",
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3239121107.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Prediction_Prob'] = ao.max(axis=1)\n",
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3239121107.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Ep_Unc'] = t_ep\n",
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3239121107.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Alea_Unc'] = t_al\n",
      "C:\\Users\\datascience-gpu\\AppData\\Local\\Temp\\ipykernel_7308\\3239121107.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Sb_Unc'] = t_sb\n"
     ]
    }
   ],
   "source": [
    "val_output_df = inference(model, val_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_output_df.to_csv(\"val_inference_samp20.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
